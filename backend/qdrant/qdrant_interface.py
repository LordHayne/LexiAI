import logging
import time
from datetime import datetime, timedelta, timezone
from uuid import UUID, uuid4
from typing import List, Optional, Union

from qdrant_client.models import PointStruct, Filter, FieldCondition, MatchValue, Range, ScoredPoint
from backend.models.memory_entry import MemoryEntry
from backend.qdrant.client_wrapper import (
    get_qdrant_client,
    safe_upsert,
    safe_search,
    safe_delete,
    safe_scroll,
    safe_set_payload
)
from backend.embeddings.embedding_cache import cached_embed_query
from backend.embeddings.sparse_encoder import get_sparse_encoder
from backend.qdrant.hybrid_search import fuse_search_results

logger = logging.getLogger("QdrantMemoryInterface")

BATCH_SIZE = 100

def chunked(lst, n):
    for i in range(0, len(lst), n):
        yield lst[i:i + n]

class QdrantMemoryInterface:
    def __init__(self, collection_name: str, embeddings, qdrant_client=None):
        self.collection = collection_name
        self.embeddings = embeddings
        self._client = qdrant_client  # Optional: von auÃŸen Ã¼bergebener Client

    @property
    def client(self):
        return self._client or get_qdrant_client()  # Fallback auf eigenen Getter

    def store_entry(self, entry: MemoryEntry) -> bool:
        """
        Store a single memory entry.

        For storing multiple entries, use batch_store_entries() for better performance.

        Returns:
            bool: True if successfully stored, False if ignored (e.g., too short)
        """
        try:
            content = entry.content.strip()
            if len(content) < 10:
                logger.info(f"Ignored short memory: '{content}'")
                return False
            # Use cached embedding for performance
            vector = entry.embedding or cached_embed_query(self.embeddings, content)
            point = PointStruct(
                id=str(entry.id),
                vector=vector,
                payload={k: v for k, v in {
                    "content": content,
                    "user_id": entry.user_id,  # USER ISOLATION: Store user_id in payload
                    "timestamp": entry.timestamp.isoformat(),
                    "category": entry.category,
                    "tags": entry.tags,
                    "source": entry.source,
                    "relevance": entry.relevance
                }.items() if v is not None}
            )
            # Nutze safe_upsert mit Retry-Mechanismus
            safe_upsert(collection_name=self.collection, points=[point])
            logger.info(f"Entry stored: {entry.id}")
            return True
        except Exception as e:
            logger.exception(f"Error storing entry {entry.id}: {e}")
            raise

    def batch_store_entries(self, entries: List[MemoryEntry], embed_missing: bool = True) -> int:
        """
        Store multiple memory entries efficiently using batching.

        PERFORMANCE: 5-10x faster than individual store_entry() calls for bulk inserts.

        Args:
            entries: List of MemoryEntry objects to store
            embed_missing: Whether to compute embeddings for entries without them

        Returns:
            Number of successfully stored entries

        Example:
            >>> entries = [MemoryEntry(...), MemoryEntry(...), ...]
            >>> stored = interface.batch_store_entries(entries)
            >>> print(f"Stored {stored}/{len(entries)} entries")
        """
        if not entries:
            return 0

        try:
            points = []
            stored_count = 0

            for entry in entries:
                try:
                    content = entry.content.strip()
                    if len(content) < 10:
                        logger.debug(f"Skipped short memory: '{content[:20]}...'")
                        continue

                    # Use existing embedding or compute if needed (with caching)
                    if entry.embedding:
                        vector = entry.embedding
                    elif embed_missing:
                        vector = cached_embed_query(self.embeddings, content)
                    else:
                        logger.warning(f"Skipped entry {entry.id}: no embedding and embed_missing=False")
                        continue

                    point = PointStruct(
                        id=str(entry.id),
                        vector=vector,
                        payload={k: v for k, v in {
                            "content": content,
                            "timestamp": entry.timestamp.isoformat(),
                            "category": entry.category,
                            "tags": entry.tags,
                            "source": entry.source,
                            "relevance": entry.relevance
                        }.items() if v is not None}
                    )
                    points.append(point)

                except Exception as e:
                    logger.warning(f"Failed to prepare entry {entry.id}: {e}")
                    continue

            # Store in batches for optimal performance
            if not points:
                logger.info("No valid entries to store")
                return 0

            for batch in chunked(points, BATCH_SIZE):
                try:
                    safe_upsert(collection_name=self.collection, points=batch)
                    stored_count += len(batch)
                    logger.debug(f"Batch stored: {len(batch)} entries")
                except Exception as e:
                    logger.error(f"Failed to store batch of {len(batch)} entries: {e}")
                    # Continue with next batch instead of failing completely

            logger.info(f"âœ… Batch stored {stored_count}/{len(entries)} entries ({len(points)} prepared)")
            return stored_count

        except Exception as e:
            logger.exception(f"Error in batch store: {e}")
            raise

    def delete_entry(self, uuid: UUID):
        try:
            # Nutze safe_delete mit Retry-Mechanismus
            safe_delete(collection_name=self.collection, points_selector=[str(uuid)])
            logger.info(f"Entry deleted: {uuid}")
        except Exception as e:
            logger.exception(f"Error deleting entry {uuid}: {e}")
            raise

    def get_all_by_category(self, category: str) -> List[ScoredPoint]:
        try:
            # Nutze safe_scroll mit Retry-Mechanismus
            scroll_result = safe_scroll(
                collection_name=self.collection,
                scroll_filter=Filter(
                    must=[FieldCondition(key="category", match=MatchValue(value=category))]
                ),
                with_payload=True,
                limit=10000
            )
            return getattr(scroll_result, "points", [])
        except Exception as e:
            logger.exception(f"Error retrieving category '{category}': {e}")
            return []

    def update_entry_metadata(self, entry_id: Union[str, UUID], metadata: dict) -> bool:
        try:
            filtered_metadata = {k: v for k, v in metadata.items() if v is not None}
            # Nutze safe_set_payload mit Retry-Mechanismus
            safe_set_payload(
                collection_name=self.collection,
                points=[str(entry_id)],
                payload=filtered_metadata
            )
            logger.info(f"Updated metadata for entry: {entry_id}")
            return True
        except Exception as e:
            logger.exception(f"Error updating metadata for {entry_id}: {e}")
            return False

    def get_all_entries(self, with_vectors: bool = True, limit: Optional[int] = None) -> List[MemoryEntry]:
        """
        Retrieve all entries from the collection with automatic pagination.

        SCALABILITY: Automatically handles collections with >10k entries via pagination.

        Args:
            with_vectors: Include embedding vectors in results
            limit: Optional maximum number of entries (None = all entries)

        Returns:
            List of MemoryEntry objects

        Example:
            >>> # Get all entries (automatic pagination)
            >>> all_entries = interface.get_all_entries()
            >>>
            >>> # Get first 5000 entries
            >>> entries = interface.get_all_entries(limit=5000)
        """
        try:
            entries = []
            offset = None
            batch_size = 1000  # Optimal batch size for pagination
            total_retrieved = 0

            while True:
                # Calculate batch size for this iteration
                if limit is not None:
                    remaining = limit - total_retrieved
                    if remaining <= 0:
                        break
                    current_batch_size = min(batch_size, remaining)
                else:
                    current_batch_size = batch_size

                # Scroll with offset for pagination
                scroll_result = safe_scroll(
                    collection_name=self.collection,
                    scroll_filter=None,
                    with_payload=True,
                    with_vectors=with_vectors,
                    limit=current_batch_size,
                    offset=offset
                )

                # Handle scroll result format
                if isinstance(scroll_result, tuple):
                    points, next_offset = scroll_result
                else:
                    points = getattr(scroll_result, "points", [])
                    next_offset = getattr(scroll_result, "next_page_offset", None)

                # No more results
                if not points:
                    break

                # Process batch
                for point in points:
                    payload = point.payload or {}

                    # Extract vector if available
                    vector = None
                    if with_vectors and hasattr(point, 'vector'):
                        vector = point.vector

                    entries.append(MemoryEntry(
                        id=point.id,
                        content=payload.get("content", ""),
                        timestamp=datetime.fromisoformat(payload.get("timestamp", datetime.now().isoformat())),
                        user_id=payload.get("user_id", "default"),
                        category=payload.get("category"),
                        tags=payload.get("tags", []),
                        source=payload.get("source"),
                        relevance=payload.get("relevance"),
                        embedding=vector,
                        is_meta_knowledge=payload.get("is_meta_knowledge", False),
                        source_memory_ids=payload.get("source_memory_ids", []),
                        synthesis_timestamp=payload.get("synthesis_timestamp"),
                        superseded=payload.get("superseded", False),
                        superseded_at=payload.get("superseded_at"),
                        superseded_by=payload.get("superseded_by"),
                        meta_topic=payload.get("meta_topic")
                    ))

                total_retrieved += len(points)

                # Check if we should continue
                if next_offset is None or (limit is not None and total_retrieved >= limit):
                    break

                offset = next_offset

            logger.info(f"Retrieved {len(entries)} entries (pagination: {(len(entries) // batch_size) + 1} batches)")
            return entries

        except Exception as e:
            logger.exception(f"Error retrieving all entries: {e}")
            return []

    def query_memories(self, query: str, user_id: Optional[str] = None, limit: int = 10, top_k: Optional[int] = None, score_threshold: Optional[float] = None) -> List[MemoryEntry]:
        """
        Query memories with semantic search.

        PERFORMANCE: Uses embedding cache for 3-5x speedup on repeated queries.
        QUALITY: Optional score_threshold filters out irrelevant results.

        Args:
            query: Search query text
            user_id: Optional user ID filter
            limit: Maximum number of results
            top_k: Alternative to limit (for backwards compatibility)
            score_threshold: Minimum similarity score (0.0-1.0).
                           Typical values: 0.7 (strict), 0.5 (moderate), 0.3 (lenient)
                           None = no filtering

        Returns:
            List of MemoryEntry objects sorted by relevance
        """
        start = time.time()
        try:
            # FIX: Validate user_id to prevent query injection
            if user_id:
                from backend.memory.adapter import validate_user_id
                user_id = validate_user_id(user_id)
            # Use cached embedding for performance
            embed_start = time.time()
            vector = cached_embed_query(self.embeddings, query)
            embed_time = (time.time() - embed_start) * 1000

            # Nutze safe_search mit Retry-Mechanismus
            search_start = time.time()
            result = safe_search(
                collection_name=self.collection,
                query_vector=vector,
                query_filter=Filter(must=[FieldCondition(
                    key="user_id", match=MatchValue(value=user_id)
                )]) if user_id else None,
                limit=top_k or limit,
                score_threshold=score_threshold,  # Filter by relevance
                with_payload=True
            )
            search_time = (time.time() - search_start) * 1000

            entries = []
            for point in result:
                payload = point.payload or {}
                entries.append(MemoryEntry(
                    id=point.id,
                    content=payload.get("content", ""),
                    timestamp=datetime.fromisoformat(payload.get("timestamp", datetime.now().isoformat())),
                    category=payload.get("category"),
                    tags=payload.get("tags", []),
                    source=payload.get("source"),
                    relevance=point.score or 1.0,
                    embedding=None
                ))

            total_time = (time.time() - start) * 1000

            # ðŸ” DEBUG MODE: Detailed memory retrieval logging
            logger.info(f"ðŸ” DEBUG - query_memories() called:")
            logger.info(f"  â””â”€ Query: '{query[:50]}...'")
            logger.info(f"  â””â”€ User ID Filter: {user_id}")
            logger.info(f"  â””â”€ Score Threshold: {score_threshold}")
            logger.info(f"  â””â”€ Limit: {limit}")
            logger.info(f"  â””â”€ Results Found: {len(entries)}")
            logger.info(f"  â””â”€ Timing: {total_time:.0f}ms (embed: {embed_time:.0f}ms, search: {search_time:.0f}ms)")

            if entries:
                logger.info(f"ðŸ” DEBUG - Retrieved {len(entries)} memories:")
                for i, entry in enumerate(entries, 1):
                    logger.info(f"  [{i}] Score: {entry.relevance:.3f} | Category: {entry.category or 'N/A'} | Content: '{entry.content[:60]}...'")
            else:
                logger.warning(f"âš ï¸ DEBUG - NO MEMORIES FOUND! Check:")
                logger.warning(f"  â””â”€ Does user_id='{user_id}' have any memories?")
                logger.warning(f"  â””â”€ Is score_threshold={score_threshold} too strict?")
                logger.warning(f"  â””â”€ Query embedding successful: {len(vector)} dimensions")

            return entries
        except Exception as e:
            elapsed_ms = (time.time() - start) * 1000
            logger.exception(f"Error querying memories ({elapsed_ms:.0f}ms): {e}")
            return []


    def similarity_search(self, query: str, k: int = 10, filter: Optional[dict] = None, score_threshold: Optional[float] = None) -> List[MemoryEntry]:
        """
        Similarity search with optional score filtering.

        Args:
            query: Search query
            k: Number of results
            filter: Optional filter dict
            score_threshold: Minimum similarity score (0.0-1.0)

        Returns:
            List of matching MemoryEntry objects
        """
        start = time.time()
        user_id = None
        if filter:
            try:
                user_id = filter["filter"]["must"][0]["match"]["value"]
            except Exception:
                logger.warning("Invalid filter format for similarity_search fallback.")
        result = self.query_memories(query=query, user_id=user_id, limit=k, score_threshold=score_threshold)
        elapsed_ms = (time.time() - start) * 1000
        logger.debug(f"â±ï¸ Qdrant similarity_search (k={k}): {elapsed_ms:.0f}ms")
        return result

    def count_user_memories(self, user_id: str) -> int:
        """
        Count total number of memories for a specific user.

        Args:
            user_id: User ID to count memories for

        Returns:
            Number of memories belonging to this user
        """
        try:
            # Validate user_id
            from backend.memory.adapter import validate_user_id
            user_id = validate_user_id(user_id)

            # Use scroll to count entries with user_id filter
            result = safe_scroll(
                collection_name=self.collection,
                scroll_filter=Filter(must=[FieldCondition(
                    key="user_id", match=MatchValue(value=user_id)
                )]),
                limit=1,  # We only need the count, not the actual data
                with_payload=False,  # Don't need payload for counting
                with_vectors=False   # Don't need vectors for counting
            )

            # The scroll returns a tuple: (points, next_page_offset)
            # To get accurate count, we need to scroll through all pages
            total_count = 0
            points, next_offset = result
            total_count += len(points)

            # If there are more pages, continue scrolling
            while next_offset:
                result = safe_scroll(
                    collection_name=self.collection,
                    scroll_filter=Filter(must=[FieldCondition(
                        key="user_id", match=MatchValue(value=user_id)
                    )]),
                    offset=next_offset,
                    limit=100,  # Use larger batch for efficiency
                    with_payload=False,
                    with_vectors=False
                )
                points, next_offset = result
                total_count += len(points)

            logger.debug(f"ðŸ“Š User {user_id} has {total_count} memories")
            return total_count
        except Exception as e:
            logger.warning(f"Failed to count memories for user {user_id}: {e}")
            return 0

    def add_entry(self, content: str, user_id: str, tags: Optional[List[str]] = None, metadata: Optional[dict] = None):
        """
        Add a single entry to the collection with automatic quality enhancements.

        QUALITY IMPROVEMENTS:
        - Automatic category prediction if not provided
        - Default source from tags or metadata
        - Deduplication check for identical content
        - Content validation

        PERFORMANCE: Uses embedding cache for repeated content.
        """
        try:
            # Content validation
            content = content.strip()
            if len(content) < 10:
                logger.warning(f"Skipped short content ({len(content)} chars)")
                return None

            timestamp = datetime.now(timezone.utc).isoformat()
            doc_id = str(uuid4())

            # Use cached embedding for performance
            embedding = cached_embed_query(self.embeddings, content)

            # Build base payload
            payload = {
                "content": content,
                "timestamp": timestamp,
                "user_id": user_id,
                "tags": tags or [],
                "relevance": 1.0,
            }

            # Merge metadata (but we'll enhance it)
            if metadata:
                payload.update(metadata)

            # QUALITY ENHANCEMENT 1: Auto-predict category if missing
            if "category" not in payload or payload.get("category") is None:
                try:
                    from backend.memory.memory_bootstrap import get_predictor
                    predictor = get_predictor()
                    if predictor and hasattr(predictor, 'predict_category'):
                        predicted_category = predictor.predict_category(content)
                        payload["category"] = predicted_category
                        logger.debug(f"Auto-predicted category: {predicted_category}")
                except Exception as e:
                    logger.debug(f"Category prediction skipped: {e}")
                    payload["category"] = "uncategorized"

            # QUALITY ENHANCEMENT 2: Infer source from tags or metadata
            if "source" not in payload or payload.get("source") is None:
                if "web_search" in (tags or []):
                    payload["source"] = "web_search"
                elif "chat" in (tags or []):
                    payload["source"] = "chat"
                elif "system" in (tags or []):
                    payload["source"] = "system"
                else:
                    payload["source"] = "user_input"

            # QUALITY ENHANCEMENT 3: Deduplication check (simple hash-based)
            # Check if identical content was stored in last 5 minutes
            try:
                recent_entries = self.query_memories(
                    content[:100],  # Use first 100 chars for similarity
                    user_id=user_id,
                    limit=3,
                    score_threshold=0.98  # Very high threshold = near-identical
                )
                for entry in recent_entries:
                    if entry.content == content:
                        time_diff = datetime.now(timezone.utc) - entry.timestamp
                        if time_diff.total_seconds() < 300:  # 5 minutes
                            logger.info(f"Skipped duplicate content (stored {time_diff.total_seconds():.0f}s ago)")
                            return entry.id  # Return existing ID
            except Exception as e:
                logger.debug(f"Deduplication check skipped: {e}")

            point = PointStruct(
                id=doc_id,
                vector=embedding,
                payload={k: v for k, v in payload.items() if v is not None}
            )

            # Nutze safe_upsert mit Retry-Mechanismus
            safe_upsert(collection_name=self.collection, points=[point])
            logger.info(f"Entry added: {doc_id} (category={payload.get('category')}, source={payload.get('source')})")

            return doc_id

        except Exception as e:
            logger.error(f"Failed to add entry: {e}")
            raise

    def hybrid_search(
        self,
        query: str,
        user_id: Optional[str] = None,
        limit: int = 10,
        dense_weight: float = 0.7,
        sparse_weight: float = 0.3,
        fusion_strategy: str = "rrf",
        score_threshold: Optional[float] = None
    ) -> List[MemoryEntry]:
        """
        Hybrid search combining semantic (dense) and keyword (sparse) search.

        ADVANCED: Best of both worlds - understands meaning AND finds exact terms.

        Args:
            query: Search query
            user_id: Optional user ID filter
            limit: Maximum results
            dense_weight: Weight for semantic search (0.0-1.0, default: 0.7)
            sparse_weight: Weight for keyword search (0.0-1.0, default: 0.3)
            fusion_strategy: "rrf" (Reciprocal Rank Fusion) or "weighted"
            score_threshold: Minimum similarity score for dense search

        Returns:
            Fused list of MemoryEntry objects

        Example:
            >>> # Query: "machine learning frameworks"
            >>> # Semantic finds: "neural network libraries", "AI tools"
            >>> # Keyword finds: exact "machine learning", "frameworks"
            >>> # Hybrid: combines both for best results!
            >>> results = interface.hybrid_search("machine learning frameworks")
        """
        try:
            logger.info(f"Hybrid search for: '{query}' (dense_weight={dense_weight}, sparse_weight={sparse_weight})")

            # 1. Dense (Semantic) Search
            dense_results = self.query_memories(
                query=query,
                user_id=user_id,
                limit=limit * 2,  # Over-fetch for fusion
                score_threshold=score_threshold
            )

            # Convert to (id, score) format for fusion
            dense_ranked = [(entry.id, entry.relevance or 0.0) for entry in dense_results]

            # 2. Sparse (Keyword) Search - using content-based filtering
            # Extract keywords from query
            import re
            keywords = re.findall(r'\b[a-z0-9]{2,}\b', query.lower())

            if not keywords:
                logger.warning("No keywords extracted from query - falling back to semantic-only")
                return dense_results[:limit]

            # Search for entries containing keywords in content
            # Use scroll with keyword filtering
            sparse_entries = []

            try:
                scroll_filter = {
                    "must": []
                }

                # Add user_id filter if provided
                if user_id:
                    scroll_filter["must"].append({
                        "key": "user_id",
                        "match": {"value": user_id}
                    })

                # Scroll through entries and score by keyword presence
                scroll_result = safe_scroll(
                    collection_name=self.collection,
                    scroll_filter=Filter(**scroll_filter) if scroll_filter["must"] else None,
                    with_payload=True,
                    limit=min(1000, limit * 10)  # Sample for keyword matching
                )

                points = scroll_result[0] if isinstance(scroll_result, tuple) else scroll_result.points

                # Score entries by keyword matches
                for point in points:
                    payload = point.payload or {}
                    content = payload.get("content", "").lower()

                    # Count keyword matches
                    matches = sum(1 for keyword in keywords if keyword in content)

                    if matches > 0:
                        # Simple TF-based score
                        score = matches / len(keywords)  # Proportion of keywords matched

                        sparse_entries.append((point.id, score))

                # Sort by score
                sparse_entries.sort(key=lambda x: x[1], reverse=True)
                sparse_ranked = sparse_entries[:limit * 2]  # Top results for fusion

                logger.debug(f"Keyword search found {len(sparse_ranked)} matches for keywords: {keywords}")

            except Exception as e:
                logger.warning(f"Sparse search failed: {e} - using semantic-only")
                sparse_ranked = []

            # 3. Fusion
            if not sparse_ranked:
                # No keyword matches - return semantic results only
                logger.info("No keyword matches - returning semantic results only")
                return dense_results[:limit]

            # Fuse results
            fused_ranked = fuse_search_results(
                dense_results=dense_ranked,
                sparse_results=sparse_ranked,
                strategy=fusion_strategy,
                dense_weight=dense_weight,
                sparse_weight=sparse_weight
            )

            # Convert back to MemoryEntry objects
            # Create lookup dict from dense_results
            entry_lookup = {entry.id: entry for entry in dense_results}

            # For IDs not in dense_results, fetch from Qdrant
            missing_ids = [doc_id for doc_id, _ in fused_ranked if doc_id not in entry_lookup]

            if missing_ids:
                # âœ… PERFORMANCE FIX: Batch retrieve all missing IDs in one query
                # Previously: O(n) queries (one per missing ID) - 10-100x slower
                # Now: O(1) batch query - 10-100x faster for large result sets
                try:
                    logger.debug(f"Batch retrieving {len(missing_ids)} missing entries")

                    # Use Qdrant's batch retrieve() method for efficient fetching
                    points = self.client.retrieve(
                        collection_name=self.collection,
                        ids=missing_ids,
                        with_payload=True,
                        with_vectors=False  # We don't need vectors for result display
                    )

                    # Process all retrieved points
                    for point in points:
                        payload = point.payload or {}

                        entry = MemoryEntry(
                            id=point.id,
                            content=payload.get("content", ""),
                            timestamp=datetime.fromisoformat(payload.get("timestamp", datetime.now().isoformat())),
                            category=payload.get("category"),
                            tags=payload.get("tags", []),
                            source=payload.get("source"),
                            relevance=1.0
                        )

                        entry_lookup[point.id] = entry

                    logger.debug(f"Successfully batch retrieved {len(points)}/{len(missing_ids)} entries")

                except Exception as e:
                    logger.warning(f"Batch retrieve failed for {len(missing_ids)} entries: {e}")
                    # Fallback: Log the error but continue with available entries
                    # This prevents total failure if some IDs are invalid

            # Build final result list
            fused_entries = []
            for doc_id, fused_score in fused_ranked[:limit]:
                if doc_id in entry_lookup:
                    entry = entry_lookup[doc_id]
                    # Update relevance with fused score
                    entry.relevance = fused_score
                    fused_entries.append(entry)

            logger.info(f"Hybrid search returned {len(fused_entries)} fused results")
            return fused_entries

        except Exception as e:
            logger.exception(f"Hybrid search failed: {e}")
            # Fallback to semantic-only
            return self.query_memories(query=query, user_id=user_id, limit=limit, score_threshold=score_threshold)
